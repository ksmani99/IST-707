bd = read.csv("C:/Users/Administrator/Documents/MSDatascience/IST707/Week3/bankdata_csv_all.csv")
str(bd)
require(pacman)
p_load(dplyr, foreign, RWeka, arules, arulesViz)
install.packages("RWeka")
setwd('C:/Users/Administrator/Documents/MSDatascience/IST707/Week3')
titanicDat <- read.arff('titanic-train-final.arff')
for(i in 1:ncol(titanicDat)){
if (class(titanicDat[[i]]) == "numeric"){
titanicDat[[i]] <- factor(titanicDat[[i]])
}
}
# 1. Using Apriori
# Set confidence (C) to 0.8 and measurement (T) to Lift (1)
titanicRules <- Apriori(titanicDat, control = Weka_control(C = 0.8, T = 1))
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
# Calculate the Euclidean distance, Manhattan distance, and cosine similarity between the two tuples.
a <- c(22, 1, 42, 10)
b <- c(20, 0, 36, 8)
# Euclidean distance
dist(rbind(a, b))
eucl.dist <- function(a, b) sqrt(sum((a - b ) ^ 2))
eucl.dist(a, b)
# Manhattan distance
manh.dist <- function(a, b){
dist <- abs(a - b)
dist <- sum(dist)
return(dist)
}
manh.dist(a, b)
# Cosine similarity
library(tcR)
cosine.similarity(a, b)
# Cosine similarity
install.packages("tcR")
library(tcR)
cosine.similarity(a, b)
# Calculate the Euclidean distance, Manhattan distance, and cosine similarity between the two tuples.
a <- c(22, 1, 42, 10)
b <- c(20, 0, 36, 8)
# Euclidean distance
dist(rbind(a, b))
eucl.dist <- function(a, b) sqrt(sum((a - b ) ^ 2))
eucl.dist(a, b)
# Manhattan distance
manh.dist <- function(a, b){
dist <- abs(a - b)
dist <- sum(dist)
return(dist)
}
manh.dist(a, b)
# Cosine similarity
install.packages("tcR")
library(tcR)
cosine.similarity(a, b)
# Calculate the Euclidean distance, Manhattan distance, and cosine similarity between the two tuples.
a <- c(22, 1, 42, 10)
b <- c(20, 0, 36, 8)
# Euclidean distance
dist(rbind(a, b))
eucl.dist <- function(a, b) sqrt(sum((a - b ) ^ 2))
eucl.dist(a, b)
# Manhattan distance
manh.dist <- function(a, b){
dist <- abs(a - b)
dist <- sum(dist)
return(dist)
}
manh.dist(a, b)
# Cosine similarity
#install.packages("tcR")
library(tcR)
cosine.similarity(a, b)
# K-means and clustering
require(stats)
#zoo data
zoo <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/zoo.csv')
cleanZoo <- zoo[, 2:18]
# Run k-means three times with a different centroid each time.
set.seed(1)
k1 <- kmeans(cleanZoo, 3)
plotcluster(cleanZoo, k1$cluster)
# Calculate the SSE per number of clusters
wss = kmeans(cleanZoo, centers = 1)$tot.withinss
for(i in 2:17){
wss[i] = kmeans(cleanZoo, centers = i)$tot.withinss
}
sse = data.frame(c(1:17), c(wss))
colnames(sse) <- c('Clusters', 'SSE')
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'blue') %>% layer_lines()
install.packages("psych")
library(psych)
# K-means and clustering
require(stats)
#install.packages("psych")
#library(psych)
#zoo data
zoo <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/zoo.csv')
cleanZoo <- zoo[, 2:18]
install.packages(plotcluster)
# Run k-means three times with a different centroid each time.
set.seed(1)
install.packages("plotCluster")
install.packages("plotcluster")
# Run k-means three times with a different centroid each time.
set.seed(1)
k1 <- kmeans(cleanZoo, 3)
plotcluster(cleanZoo, k1$cluster)
# Run k-means three times with a different centroid each time.
set.seed(1)
k1 <- kmeans(cleanZoo, 3)
plotCluster(cleanZoo, k1$cluster)
library(cluster)
#library(cluster)
#zoo data
zoo <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/zoo.csv')
cleanZoo <- zoo[, 2:18]
install.packages(cluster.)
# Run k-means three times with a different centroid each time.
set.seed(1)
k1 <- kmeans(cleanZoo, 3)
plotcluster(cleanZoo, k1$cluster)
install.packages("fpc")
#library(cluster)
#library(fpc)
#zoo data
zoo <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/zoo.csv')
cleanZoo <- zoo[, 2:18]
install.packages(cluster.)
# Run k-means three times with a different centroid each time.
set.seed(1)
k1 <- kmeans(cleanZoo, 3)
plotcluster(cleanZoo, k1$cluster)
library(fpc)
library(fpc)
library(fpc)
library(fpc)
#library(cluster)
#library(fpc)
#zoo data
zoo <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/zoo.csv')
cleanZoo <- zoo[, 2:18]
install.packages(cluster.)
# Run k-means three times with a different centroid each time.
set.seed(1)
# K-means and clustering
require(stats)
#library(cluster)
#library(fpc)
#zoo data
zoo <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/zoo.csv')
cleanZoo <- zoo[, 2:18]
# Run k-means three times with a different centroid each time.
set.seed(1)
k1 <- kmeans(cleanZoo, 3)
plotcluster(cleanZoo, k1$cluster)
library(cluster)
# Calculate the SSE per number of clusters
wss = kmeans(cleanZoo, centers = 1)$tot.withinss
for(i in 2:17){
wss[i] = kmeans(cleanZoo, centers = i)$tot.withinss
}
sse = data.frame(c(1:17), c(wss))
colnames(sse) <- c('Clusters', 'SSE')
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'blue') %>% layer_lines()
install.packages("ggvis")
#library(cluster)
#library(fpc)
library(ggvis)
#zoo data
datazoo <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/zoo.csv')
datazoocleaned <- datazoo[, 2:18]
# Run k-means three times with a different centroid each time.
set.seed(1)
k1 <- kmeans(datazoocleaned, 3)
plotcluster(datazoocleaned, k1$cluster)
library(cluster)
# Calculate the SSE per number of clusters
wss = kmeans(datazoocleaned, centers = 1)$tot.withinss
# Calculate the SSE per number of clusters
wss = kmeans(datazoocleaned, centers = 1)$tot.withinss
for(i in 2:17){
wss[i] = kmeans(datazoocleaned, centers = i)$tot.withinss
}
sse = data.frame(c(1:17), c(wss))
colnames(sse) <- c('Clusters', 'SSE')
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'blue') %>% layer_lines()
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
colnames(sse) <- c('Clusters', 'SSE')
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'red') %>% layer_lines()
# K-means and clustering
#=======================#
#library(fpc)  - Required for clusterplot
#library(ggvis) - required for visualization
#zoo data
datazoo <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/zoo.csv')
datazoocleaned <- datazoo[, 2:18]
# Run k-means three times with a different centroid each time.
set.seed(1)
k1 <- kmeans(datazoocleaned, 3)
plotcluster(datazoocleaned, k1$cluster)
# Calculate the SSE per number of clusters
wss = kmeans(datazoocleaned, centers = 1)$tot.withinss
for(i in 2:17){
wss[i] = kmeans(datazoocleaned, centers = i)$tot.withinss
}
sse = data.frame(c(1:17), c(wss))
colnames(sse) <- c('Clusters', 'SSE')
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'red') %>% layer_lines()
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = T, shade = F, labels = 0, lines = 0)
install.packages("plotCluster")
# K-means and clustering
#=======================#
#library(fpc)  - Required for clusterplot
#library(ggvis) - required for visualization
#zoo data
datazoo <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/zoo.csv')
datazoocleaned <- datazoo[, 2:18]
# Run k-means three times with a different centroid each time.
set.seed(1)
k1 <- kmeans(datazoocleaned, 3)
plotcluster(datazoocleaned, k1$cluster)
# Calculate the SSE per number of clusters
wss = kmeans(datazoocleaned, centers = 1)$tot.withinss
for(i in 2:17){
wss[i] = kmeans(datazoocleaned, centers = i)$tot.withinss
}
sse = data.frame(c(1:17), c(wss))
colnames(sse) <- c('Clusters', 'SSE')
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'red') %>% layer_lines()
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = T, shade = F, labels = 0, lines = 0)
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = TRUE, shade = TRUE, labels = 2, lines = 0)
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = TRUE, shade = FALSE, labels = 2, lines = 0)
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = TRUE, shade = FALSE, labels = 1, lines = 0)
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = TRUE, shade = FALSE, labels = 1, lines = 1)
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'red') %>% layer_lines()
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = TRUE, shade = FALSE, labels = 1, lines = 2)
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = TRUE, shade = FALSE, labels = 1, lines =0, plotchar = TRUE)
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'red') %>% layer_lines()
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = TRUE, shade = FALSE, labels = 1, lines =0, plotchar = TRUE)
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'red') %>% layer_lines()
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = TRUE, shade = FALSE, labels = 2, col.txt = TRUE, lines =0)
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'red') %>% layer_lines()
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = TRUE, shade = FALSE, labels = 2, col.txt = TRUE, lines =0, plotchar = TRUE)
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'red') %>% layer_lines()
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = F, shade = FALSE, labels = 0, lines = 0)
# K-means and clustering
#=======================#
#library(fpc)  - Required for clusterplot
#library(ggvis) - required for visualization
#zoo data
datazoo <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/zoo.csv')
datazoocleaned <- datazoo[, 2:18]
# Run k-means three times with a different centroid each time.
set.seed(1)
k1 <- kmeans(datazoocleaned, 3)
plotcluster(datazoocleaned, k1$cluster)
# Calculate the SSE per number of clusters
wss = kmeans(datazoocleaned, centers = 1)$tot.withinss
for(i in 2:17){
wss[i] = kmeans(datazoocleaned, centers = i)$tot.withinss
}
sse = data.frame(c(1:17), c(wss))
colnames(sse) <- c('Clusters', 'SSE')
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'red') %>% layer_lines()
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = T, shade = F, labels = 0, lines = 0)
#zoo data
datazoo <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/zoo.csv')
datazoocleaned <- datazoo[, 2:18]
# Run k-means three times with a different centroid each time.
set.seed(1)
k1 <- kmeans(datazoocleaned, 3)
plotcluster(datazoocleaned, k1$cluster)
# Calculate the SSE per number of clusters
wss = kmeans(datazoocleaned, centers = 1)$tot.withinss
for(i in 2:17){
wss[i] = kmeans(datazoocleaned, centers = i)$tot.withinss
}
sse = data.frame(c(1:17), c(wss))
colnames(sse) <- c('Clusters', 'SSE')
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'red') %>% layer_lines()
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = T, shade = F, labels = 0, lines = 0)
# K-means and clustering
#=======================#
#library(fpc)  - Required for clusterplot
#library(ggvis) - required for visualization
#zoo data
datazoo <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/zoo.csv')
datazoocleaned <- datazoo[, 2:18]
# Run k-means three times with a different centroid each time.
set.seed(1)
k1 <- kmeans(datazoocleaned, 3)
plotcluster(datazoocleaned, k1$cluster)
# Calculate the SSE per number of clusters
wss = kmeans(datazoocleaned, centers = 1)$tot.withinss
for(i in 2:17){
wss[i] = kmeans(datazoocleaned, centers = i)$tot.withinss
}
sse = data.frame(c(1:17), c(wss))
colnames(sse) <- c('Clusters', 'SSE')
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'red') %>% layer_lines()
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = T, shade = F, labels = 0, lines = 0)
#zoo data
datazoo <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/zoo.csv')
datazoocleaned <- datazoo[, 2:18]
# Run k-means three times with a different centroid each time.
set.seed(1)
k1 <- kmeans(datazoocleaned, 3)
plotcluster(datazoocleaned, k1$cluster)
# Calculate the SSE per number of clusters
wss = kmeans(datazoocleaned, centers = 1)$tot.withinss
for(i in 2:17){
wss[i] = kmeans(datazoocleaned, centers = i)$tot.withinss
}
sse = data.frame(c(1:17), c(wss))
colnames(sse) <- c('Clusters', 'SSE')
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'red') %>% layer_lines()
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = T, shade = F, labels = 0, lines = 0)
#zoo data
datazoo <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/zoo.csv')
datazoocleaned <- datazoo[, 2:18]
# Run k-means three times with a different centroid each time.
set.seed(1)
k1 <- kmeans(datazoocleaned, 3)
plotcluster(datazoocleaned, k1$cluster)
# Calculate the SSE per number of clusters
wss = kmeans(datazoocleaned, centers = 1)$tot.withinss
for(i in 2:17){
wss[i] = kmeans(datazoocleaned, centers = i)$tot.withinss
}
sse = data.frame(c(1:17), c(wss))
colnames(sse) <- c('Clusters', 'SSE')
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'red') %>% layer_lines()
for(i in 2:17){
wss[i] = kmeans(datazoocleaned, centers = i)$tot.withinss
}
sse = data.frame(c(1:17), c(wss))
colnames(sse) <- c('Clusters', 'SSE')
# Visualize the 'elbow' plot to maximize # of clusters while minimizing SSE
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'red') %>% layer_lines()
# Create cluster and plot.
k2 <- kmeans(datazoocleaned, 9)
clusplot(datazoocleaned, k1$cluster, color = T, shade = F, labels = 0, lines = 0)
---
title: "Kmeans"
author: "Subbu Kandhaswamy"
date: "10/30/2019"
output: html_document
---
knitr::opts_chunk$set(echo = TRUE)
datazoo <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/zoo.csv')
str(datazoo)
model_r <- kmeans(datazoocleaned,7)
model_r
model_r$centers
cluster_assignment <- data.frame(datazoo,model_r$cluster)
View(cluster_assignment)
plot(datazoo$type ~ jitter(model_r$cluster,1),pch=21,col=as.factor(datazoo$milk))
library(cluster)
clusplot(datazoocleaned,model_r$cluster, color=TRUE, shade = T, labels = 2, lines = 0) # plot clusters
d=dist(as.matrix(datazoocleaned))
hc=hclust(d)
plot(hc)
plot(hc)
library(RWeka)
model_rweka <- SimpleKMeans(datazoocleaned, control = Weka_control(N=7, U=500, S=100))
datazoocleaned <- datazoo[, 2:18]
library(RWeka)
model_rweka <- SimpleKMeans(datazoocleaned, control=Weka_control(N=7,  S=100))
model_rweka
read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/iris.csv')
read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/iris.csv')
read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/iris.csv')
irisdata <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/iris.csv')
datairis <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/iris.csv')
library(RWeka)
random = sample(1:dim(iris)[1],50)
datairis = iris[random,]
datairis$Species=NULL
datairis
iris$Species
plot(hcluster,hang=-1,labels=iris$Species[random])
knitr::opts_chunk$set(echo = TRUE)
require(stats)
require(cluster)
require(dplyr)
require(ggvis)
require(tidyr)
require(stats)
require(cluster)
require(dplyr)
require(ggvis)
require(tidyr)
```{r tidy = TRUE}
feds <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week4/fedPapers85.csv')
# Exploring the data set.
dim(feds)
# If we check the six first rows, we can see that the first two columns refer to the author of the text and the text itself. Columns three and so forth refer to the frequency of letters occurring in the text.
head(feds)
# Having said that, the first two columns are nominal, while the remaining columns are numeric.
str(feds[, 1:6])
table(sapply(feds, class))
# Check how many unique authors there are. We disregard disputed papers as an author as we want to identify who the real author might be.
unique(feds$author)
# Now, we'll create a new data set in which the author and text columns are removed.
fedsClean <- feds[, 3:72]
## Cluster analysis.
We shall do three types of clustering analysis: k-means and HAC.
### 1. k-means
We'll start using the k-means algorithm by selecting three centroids. The reason why we choose three centroids is because there are four unique authors: Hamilton, Madison, and Jay. We want to check to which of these groups the 11 disputed papers belongs to.
```{r tidy = TRUE}
# Check how many unique authors there are. We disregard disputed papers as an author as we want to identify who the real author might be.
unique(feds$author)
# Since there are three authors, we will work with three centroids.
# Now, we'll create a new data set in which the author and text columns are removed.
# Now, we'll create a new data set in which the author and text columns are removed.
fedsClean <- feds[, 3:72]
set.seed(825)
k1 <- kmeans(fedsClean, 3)
k1$size
# We seem to have run into an unexpected grouping as the sizes of the clusters are not what was expected. Let's plot the clusters to see how they compare.
clusplot(fedsClean, k1$cluster, color = T, shade = F, labels = 0, lines = 0)
## Cluster analysis.
We shall do three types of clustering analysis: k-means and HAC.
### 1. k-means
We'll start using the k-means algorithm by selecting three centroids. The reason why we choose three centroids is because there are four unique authors: Hamilton, Madison, and Jay. We want to check to which of these groups the 11 disputed papers belongs to.
```{r tidy = TRUE}
# Check how many unique authors there are. We disregard disputed papers as an author as we want to identify who the real author might be.
unique(feds$author)
# Since there are three authors, we will work with three centroids.
# We can clearly see that there is overlap between the four clusters. Perhaps a better approach would be to do an elbow chart and determine the ideal number of centroids to use.
wss <- kmeans(fedsClean, centers = 1)$tot.withinss
for (i in 3:72){
wss[i] <- kmeans(fedsClean, centers = i)$tot.withinss
}
sse <- data.frame(c(1:72), c(wss))
colnames(sse) <- c('Clusters', 'SSE')
wss <- kmeans(fedsClean, centers = 1)$tot.withinss
for (i in 3:72){
wss[i] <- kmeans(fedsClean, centers = i)$tot.withinss
}
sse <- data.frame(c(1:72), c(wss))
colnames(sse) <- c('Clusters', 'SSE')
sse %>% ggvis(~Clusters, ~SSE) %>% layer_points(fill := 'blue') %>% layer_lines()
fedsCluster <- cbind(feds, k1$cluster)
colnames(fedsCluster)[73] <- 'cluster'
fedsClusterClean <- fedsCluster %>% group_by(author, cluster) %>% summarise(number = n())
spread(fedsClusterClean, key = cluster, value = number)
clusComp <- hclust(dist(feds[, 3:72]), method = 'complete')
clusAvg <- hclust(dist(feds[, 3:72]), method = 'average')
clusComp
clusAvg
# Time to graph the clusters and see how they compare to the k-means algorithm.
plot(clusComp, hang = -1, cex = 0.6, main = "Federalist Papers Cluster - Complete", label = feds$author)
plot(clusAvg, hang = -1, cex = 0.6, main = "Federalist Papers Cluster - Average", label = feds$author)
unlink('C:/Users/Administrator/Desktop/HW-4-Subbu-Kandhaswamy-IST707_cache', recursive = TRUE)
install.packages("latexpdf")
unlink('~/MSDatascience/IST707/Week4/HW-4-Subbu-Kandhaswamy-IST707_cache', recursive = TRUE)
unlink('~/MSDatascience/IST707/Week4/HW-4-Subbu-Kandhaswamy-IST707_cache', recursive = TRUE)
