---
title: "Homework-5"
author: "Subbu Kandhaswamy"
date: "11/6/2019"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:



# Homework Instructions - Last week we used Clustering and in this we are using the decision tree algorithm to solve the disputed essay problem. 
  
## 1. Data Preparation  
We'll start by loading the necesary packages to work with data partitioning, model design, and graphics outputs. 
```{r tidy = TRUE}
require(caret)
require(dplyr)
require(ggplot2)
require(rpart)
require(rpart.plot)
require(e1071)
```

Now loading the data. Since this is the same dataset as last week's homework, we will skip summary, structure, and head.  

```{r tidy = TRUE}
dat <- read.csv('C:/Users/Administrator/Documents/MSDatascience/IST707/Week5/fedPapers85.csv')
```

Then separate the dataset into 3 groups: Training,  Testing  &  Verification set 

Now extract the disputed papers from the set as we want to know if the model can predict to who these belong to. The training set will consist of 2/3 of the dataset after removing  verification set; and the test set will be the remaining 1/3. 


```{r tidy = TRUE}
fed <- dat %>% filter(author != 'dispt')
fed$author <- as.factor(as.character(fed$author))

# Randomly select 2/3 of the dataset - these will be the training set. 
split <- sample(nrow(fed), nrow(fed) * 2/3)
train <- fed[split, ]
test <- fed[-split, ]

# The verification set,
ver <- dat %>% filter(author == 'dispt')
```



## 2. Build and train the model. 
```{r tidy = TRUE}
# We'll create a first tree using rpart. and we exclude  filename variable considering the type (factor)  and doesn't add much to the decision tree. 
feds.tree <- rpart(author ~ . - filename, data = train, method = 'class', control = rpart.control(cp = 0))

# Plotting to see how the model works vefore pruning
rpart.plot(feds.tree)

# And also look under the hood.
summary(feds.tree)
```

Notice the model has a starting node as Hamilton - 71 percent of the texts in the training set belong to Hamilton. From there, the model asks whether  the word 'upon' is used above 1.9 percent. If the 'upon' appears more frequently, then the model asigns the paper to Hamilton. 
But, if 'upon' is used less than 1.9 percent, then the model immediately accredit the text to Madison. 
Also notice that at this point in the model - no papers are assigned to Jay or to the dual Hamilton-Madison papers. The model will have to be tuned to fix this. 
Furthermore, when checking the model summary, we find that at two splits, the relative error of the model is 0.429. This is not desirable as we want a more accurate model. 

```{r tidy = TRUE}
# We'll set a minimum split of 10 papers in a bucket, and a max depth of 4 leaf nodes, and check how the model works from there. 
feds.tree2 <- rpart(author ~ . - filename, data = train, method = 'class', control = rpart.control(cp = 0, minsplit = 10, maxdepth = 4))

rpart.plot(feds.tree2)

summary(feds.tree2)
```

We can see that this tree is much more accurate, producing a relative error of 0.143 at two splits. We also see that the tree further elaborates beyond the Madison leaf node, going so far as to identify the Jay papers. 
Returning to the Madison leaf node, if the author uses 'been' less than 5.3 percent of the time, then it is a Jay paper. Anything more is still attributed to Madison.
Unfortunately, the model is still failing to determine the Hamilton-Madison papers, and we see that it's struggling to correctly identify the Jay papers, as only 67 percent of the papers attributed to Jay are in fact his. 

```{r tidy = TRUE}
# Let's reduce the minimum split to five, while also increasing the max depth to five. 
feds.tree3 <- rpart(author ~ . - filename, data = train, method = 'class', control = rpart.control(cp = 0, minsplit = 5, maxdepth = 5))

rpart.plot(feds.tree3, cex = 0.8)

summary(feds.tree3)
```

This model gives us a relative error of 0.071, which is amamzing. Most of the papers are being assigned correctly to Hamilton, Jay, and Madison. However, at the first Jay node, if the word 'also' appears less than one percent of the time, the model assigns the text to Hamilton with a 50 percent probability of getting it right. We can assume that both Hamilton's and the Hamilton Madison papers are being classified here. 

Let's check the cp plot and see how the model works on the test set. 
```{r tidy = TRUE}
plotcp(feds.tree3)

# Now we'll use this model to predict the test set. 
test_pred <- data.frame(predict(feds.tree3, newdata = test))

# Some data transformation needs to be done to check the confusion matrix.
results <- test_pred %>% mutate(results = ifelse(Madison == 1, 'Madison', ifelse(Hamilton == 1, 'Hamilton', ifelse(Jay == 1, 'Jay', 'HM'))))
# Lets introduce some bias here to  explicitly statethat if none of the rules apply to the results, then the result should be HM This will cause some texts which are 50-50 to be classified as HM. 

row.names(test) <- NULL
testResult <- test %>% bind_cols(results)
testResult$results <- as.factor(testResult$results)

confusionMatrix(testResult$result, testResult$author)

```

Reviewing the confusion matrix, we could confirm that that the model correctly assigns the Hamilt, Jay , and Madison texts to each author. Also confirms the issue lies with the HM papers (HM or disputed papers). 


## 3. Prediction
But, we have one more test to actually see how the model fares: can the model predict who the disputed texts belong to? 

```{r tidy = TRUE}

ver_pred <- predict(feds.tree3, newdata = ver)

# We'll skip the confusion matrix in this case and just check the results.
ver_pred

```
So Out of the 11 disputed papers, the model classifies 3 as authored by Madison, one by Jay, and the remaining as a 50/50 chance being authored either by Hamilton or Hamilton & Madison.
In comparison, last week using clustering technique was less effective, where there was less evidence to determine the authors where the average method showed that some of the papers were more likely to be authored by Hamilton, some by Madison, and a couple that couldnâ€™t  be determined since the language used by the authors were closely similar.
